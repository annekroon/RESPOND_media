{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247fe4d2-16ae-4a47-a01e-65b752353c52",
   "metadata": {},
   "source": [
    "# Political Corruption Article Sampling, Translation, and LLM Annotation Support\n",
    "\n",
    "This notebook performs a multi-step pipeline to support human annotation of news articles for political corruption. It:\n",
    "\n",
    "1. Loads and balances samples of news articles across selected countries.\n",
    "2. Translates non-English articles into English using the Gemma3 or NLLB model, with chunking and retry logic for robustness.\n",
    "3. Uses an LLM to identify whether political corruption is a central theme, highlighting relevant sentences, providing a rationale, tentative label, and confidence score.\n",
    "4. Highlights keywords and model-identified sentences to assist annotators.\n",
    "5. Outputs a CSV with translations, model suggestions, and fields for human labeling.\n",
    "\n",
    "The final output is designed to streamline annotation workflows and improve label consistency and quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76301ee6-366c-417c-9e33-1ced95358684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/2 (chars: 1479)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a925094d2ba6416cb66dea973fa08122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 2/2 (chars: 499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   5%|▌         | 1/20 [00:04<01:23,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/3 (chars: 1322)\n",
      "Translating chunk 2/3 (chars: 1445)\n",
      "Translating chunk 3/3 (chars: 1463)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  10%|█         | 2/20 [00:14<02:19,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/3 (chars: 1392)\n",
      "Translating chunk 2/3 (chars: 1373)\n",
      "Translating chunk 3/3 (chars: 258)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  15%|█▌        | 3/20 [00:22<02:16,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/1 (chars: 889)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  25%|██▌       | 5/20 [00:24<00:59,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/1 (chars: 1479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  30%|███       | 6/20 [00:28<00:55,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/2 (chars: 1500)\n",
      "Translating chunk 2/2 (chars: 692)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  35%|███▌      | 7/20 [00:35<01:02,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/4 (chars: 1482)\n",
      "Translating chunk 2/4 (chars: 1279)\n",
      "Translating chunk 3/4 (chars: 1445)\n",
      "Translating chunk 4/4 (chars: 910)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  40%|████      | 8/20 [00:46<01:21,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/1 (chars: 1348)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  50%|█████     | 10/20 [00:50<00:45,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/5 (chars: 1283)\n",
      "Translating chunk 2/5 (chars: 1397)\n",
      "Translating chunk 3/5 (chars: 1202)\n",
      "Translating chunk 4/5 (chars: 1244)\n",
      "Translating chunk 5/5 (chars: 518)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  55%|█████▌    | 11/20 [01:05<01:03,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/2 (chars: 1364)\n",
      "Translating chunk 2/2 (chars: 358)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  70%|███████   | 14/20 [01:09<00:24,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/1 (chars: 1251)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  75%|███████▌  | 15/20 [01:12<00:19,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/3 (chars: 1432)\n",
      "Translating chunk 2/3 (chars: 1359)\n",
      "Translating chunk 3/3 (chars: 887)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  80%|████████  | 16/20 [01:19<00:18,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/1 (chars: 1381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  85%|████████▌ | 17/20 [01:23<00:13,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/6 (chars: 1125)\n",
      "Translating chunk 2/6 (chars: 1248)\n",
      "Translating chunk 3/6 (chars: 1468)\n",
      "Translating chunk 4/6 (chars: 1150)\n",
      "Translating chunk 5/6 (chars: 1442)\n",
      "Translating chunk 6/6 (chars: 237)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  90%|█████████ | 18/20 [01:39<00:14,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/4 (chars: 1280)\n",
      "Translating chunk 2/4 (chars: 1434)\n",
      "Translating chunk 3/4 (chars: 1114)\n",
      "Translating chunk 4/4 (chars: 564)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:  95%|█████████▌| 19/20 [01:50<00:08,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating chunk 1/3 (chars: 1353)\n",
      "Translating chunk 2/3 (chars: 1447)\n",
      "Translating chunk 3/3 (chars: 1169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 20/20 [01:59<00:00,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Saved translated samples to 'sample_for_annotation.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import difflib\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "# ========== Config ==========\n",
    "NEWS_FOLDER = \"/home/akroon/data/volume_2/RESPOND_NEWS\"\n",
    "SELECTED_COUNTRIES = [\"Bulgaria\", \"Italy\", \"Netherlands\", \"United_Kingdom\"]\n",
    "TOTAL_SAMPLES = 1000\n",
    "TRANSLATION_SAMPLE_SIZE = 20\n",
    "\n",
    "COUNTRY_TO_LANG = {\n",
    "    \"Bulgaria\": \"bg\",\n",
    "    \"Italy\": \"it\",\n",
    "    \"Netherlands\": \"nl\",\n",
    "    \"United_Kingdom\": \"en\"\n",
    "}\n",
    "\n",
    "# Load NLLB fallback pipeline\n",
    "nllb_translator = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\", src_lang=\"bul_BUL\", tgt_lang=\"eng_Latn\")\n",
    "\n",
    "# ========== Utility Functions ==========\n",
    "\n",
    "def truncate_text(text, max_chars=512):\n",
    "    return text if len(text) <= max_chars else text[:max_chars] + \"...\"\n",
    "\n",
    "def is_probably_same_language(original, translation):\n",
    "    non_ascii_chars = sum(1 for c in translation if ord(c) > 127)\n",
    "    return non_ascii_chars / max(len(translation), 1) > 0.3\n",
    "\n",
    "# ========== Load & Sample ==========\n",
    "\n",
    "def load_and_prepare_data(news_folder, countries):\n",
    "    all_dfs = []\n",
    "    for country in countries:\n",
    "        file_path = os.path.join(news_folder, f\"{country}_news.csv\")\n",
    "        print(f\"Loading {file_path} ...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df['year'] = df['date'].dt.year.fillna(0).astype(int)\n",
    "\n",
    "        if 'combined_text' not in df.columns:\n",
    "            df['title'] = df['title'].fillna(\"\").astype(str)\n",
    "            df['body'] = df['body'].fillna(\"\").astype(str)\n",
    "            df['combined_text'] = df['title'] + \" \" + df['body']\n",
    "\n",
    "        df['country'] = country\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def balanced_sample(df, total_samples=1000, countries=None):\n",
    "    if countries is None:\n",
    "        countries = df['country'].unique()\n",
    "\n",
    "    samples_per_country = total_samples // len(countries)\n",
    "    sampled_dfs = []\n",
    "\n",
    "    for country in countries:\n",
    "        df_country = df[df['country'] == country]\n",
    "        years = df_country['year'].unique()\n",
    "        years = years[years != 0]\n",
    "\n",
    "        if len(years) == 0:\n",
    "            sample = df_country.sample(n=min(samples_per_country, len(df_country)), random_state=42)\n",
    "            sampled_dfs.append(sample)\n",
    "            continue\n",
    "\n",
    "        total_country_articles = len(df_country)\n",
    "        samples_for_year = {\n",
    "            year: int(round(samples_per_country * (len(df_country[df_country['year'] == year]) / total_country_articles)))\n",
    "            for year in years\n",
    "        }\n",
    "\n",
    "        diff = samples_per_country - sum(samples_for_year.values())\n",
    "        if diff != 0:\n",
    "            biggest_year = max(samples_for_year, key=samples_for_year.get)\n",
    "            samples_for_year[biggest_year] += diff\n",
    "\n",
    "        samples = []\n",
    "        for year, n_samples in samples_for_year.items():\n",
    "            df_year = df_country[df_country['year'] == year]\n",
    "            n_samples = min(n_samples, len(df_year))\n",
    "            samples.append(df_year.sample(n=n_samples, random_state=42))\n",
    "\n",
    "        sampled_country = pd.concat(samples)\n",
    "        sampled_dfs.append(sampled_country)\n",
    "\n",
    "    final_sample = pd.concat(sampled_dfs).reset_index(drop=True)\n",
    "    print(f\"Sampled total {len(final_sample)} articles across {len(countries)} countries.\")\n",
    "    return final_sample\n",
    "\n",
    "# ========== Translation Logic ==========\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_CHUNK_SIZE = 1500  # max chars per chunk\n",
    "MIN_TRANSLATION_RATIO = 0.7  # minimum ratio of output length to input length to accept translation\n",
    "\n",
    "def truncate_text(text, max_chars=MAX_CHUNK_SIZE):\n",
    "    return text if len(text) <= max_chars else text[:max_chars] + \"...\"\n",
    "\n",
    "def is_probably_same_language(original, translation, threshold=0.3):\n",
    "    # heuristic: fraction of non-ascii chars in translation\n",
    "    non_ascii_chars = sum(1 for c in translation if ord(c) > 127)\n",
    "    return non_ascii_chars / max(len(translation), 1) > threshold\n",
    "\n",
    "def split_text_into_chunks(text, max_chunk_size=MAX_CHUNK_SIZE):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) + 2 <= max_chunk_size:\n",
    "            current_chunk += para + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            # forcibly split long paragraph if needed\n",
    "            while len(para) > max_chunk_size:\n",
    "                chunks.append(para[:max_chunk_size].strip())\n",
    "                para = para[max_chunk_size:]\n",
    "            current_chunk = para + \"\\n\\n\"\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def translate_chunk_with_gemma3(chunk_text, source_lang):\n",
    "    system_prompt = (\n",
    "        f\"You are a translation assistant. Translate the following {source_lang} text into English. \"\n",
    "        \"**Translate the entire text fully and exactly, do NOT shorten or summarize.** \"\n",
    "        \"Do NOT explain or paraphrase. Output ONLY the translated text.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": chunk_text}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/chat\",\n",
    "            json={\n",
    "                \"model\": \"zongwei/gemma3-translator:4b\",\n",
    "                \"messages\": messages,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        raw_translation = result.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "        # Optional cleanup\n",
    "        if raw_translation.lower().startswith(\"here’s the translation\"):\n",
    "            parts = raw_translation.split(\"\\n\\n\", 1)\n",
    "            if len(parts) == 2:\n",
    "                raw_translation = parts[1].strip()\n",
    "\n",
    "        if source_lang != \"en\" and is_probably_same_language(chunk_text, raw_translation):\n",
    "            print(\"⚠️ Suspected untranslated chunk output — marking empty\")\n",
    "            return \"\"\n",
    "\n",
    "        return raw_translation\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Translation error (Gemma3): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def translation_is_too_short(original, translated, threshold=MIN_TRANSLATION_RATIO):\n",
    "    return len(translated) < threshold * len(original)\n",
    "\n",
    "def translate_article_with_chunking(text, lang):\n",
    "    chunks = split_text_into_chunks(text, max_chunk_size=MAX_CHUNK_SIZE)\n",
    "    translated_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Translating chunk {i+1}/{len(chunks)} (chars: {len(chunk)})\")\n",
    "        translated_chunk = translate_chunk_with_gemma3(chunk, source_lang=lang)\n",
    "\n",
    "        if not translated_chunk or translation_is_too_short(chunk, translated_chunk):\n",
    "            print(\"🔁 Retry chunk with truncated input...\")\n",
    "            truncated_chunk = truncate_text(chunk, max_chars=MAX_CHUNK_SIZE)\n",
    "            translated_chunk = translate_chunk_with_gemma3(truncated_chunk, source_lang=lang)\n",
    "\n",
    "            if not translated_chunk or translation_is_too_short(chunk, translated_chunk):\n",
    "                print(f\"❌ Failed to translate chunk {i+1} properly.\")\n",
    "                translated_chunk = \"[Translation Failed]\"\n",
    "\n",
    "        translated_chunks.append(translated_chunk)\n",
    "\n",
    "    full_translation = \"\\n\\n\".join(translated_chunks)\n",
    "    return full_translation\n",
    "\n",
    "# ========== Main translation loop ==========\n",
    "\n",
    "output_rows = []\n",
    "failed_translations = []\n",
    "\n",
    "for lang, original_text in tqdm(sampled_articles, desc=\"Translating\"):\n",
    "    if lang == \"en\":\n",
    "        translated_text = original_text\n",
    "    else:\n",
    "        translated_text = translate_article_with_chunking(original_text, lang)\n",
    "\n",
    "    if not translated_text or \"[Translation Failed]\" in translated_text:\n",
    "        failed_translations.append({\n",
    "            \"lang\": lang,\n",
    "            \"original_text\": original_text,\n",
    "            \"issue\": \"Failed or empty translation\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    if is_probably_same_language(original_text, translated_text):\n",
    "        print(\"⚠️ Suspected untranslated output — marking for review\")\n",
    "        failed_translations.append({\n",
    "            \"lang\": lang,\n",
    "            \"original_text\": original_text,\n",
    "            \"translated_text\": translated_text,\n",
    "            \"issue\": \"Likely not translated\"\n",
    "        })\n",
    "\n",
    "    output_rows.append({\n",
    "        \"original_text\": original_text,\n",
    "        \"translated_text\": translated_text,\n",
    "        \"label\": \"\"\n",
    "    })\n",
    "\n",
    "# Save translated samples\n",
    "out_df = pd.DataFrame(output_rows)\n",
    "out_df.to_csv(\"sample_for_annotation.csv\", index=False)\n",
    "print(\"✅ Done! Saved translated samples to 'sample_for_annotation.csv'\")\n",
    "\n",
    "# Save failed or suspicious translations\n",
    "if failed_translations:\n",
    "    fail_df = pd.DataFrame(failed_translations)\n",
    "    fail_df.to_csv(\"failed_translations_log.csv\", index=False)\n",
    "    print(f\"⚠️ Logged {len(failed_translations)} failed/suspect translations to 'failed_translations_log.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4dd109-e284-40cd-a95a-46917a3e12ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Loading translated articles from sample_for_annotation.csv...\n",
      "🧠 Generating LLM suggestions for annotation support...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 1000/1000 [42:45<00:00,  2.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to: sample_with_llm_suggestions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from typing import List  # Add this at the top of your script\n",
    "\n",
    "\n",
    "# ========= Config ==========\n",
    "TRANSLATED_FILE = \"sample_for_annotation.csv\"\n",
    "OUTPUT_FILE = \"sample_with_llm_suggestions.csv\"\n",
    "\n",
    "LLM_ENDPOINT = \"http://localhost:11434/api/chat\"\n",
    "LLM_MODEL_NAME = \"llama3\"  # Replace if needed\n",
    "\n",
    "# ========= Prompt Builder ==========\n",
    "def build_detailed_prompt(article_text: str) -> str:\n",
    "    return f\"\"\"You are helping a human annotator identify whether a news article is primarily about **political corruption**.\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Political corruption** refers to situations where public power is misused for personal or political gain, especially in the context of political decision-making.\n",
    "\n",
    "It involves **public officials** such as:\n",
    "- Government ministers, members of parliament, or judges\n",
    "- Mayors, governors, or local council members\n",
    "- Leaders of regulatory agencies\n",
    "\n",
    "(NOTE: Do **not** consider cases involving only police chiefs, military commanders, or leaders of state-owned companies.)\n",
    "\n",
    "Common forms include:\n",
    "- **Bribery** – accepting money or gifts for influence or decisions\n",
    "- **Embezzlement** – stealing or misusing public funds\n",
    "- **Nepotism / Cronyism** – appointing unqualified relatives or friends\n",
    "- **Fraud, kickbacks, or money laundering** – illicit financial conduct\n",
    "- **Abuse of authority** – rigging elections, silencing dissent, shielding allies\n",
    "\n",
    "> These behaviors must involve public officials misusing public trust in political roles.\n",
    "\n",
    "### Task Instructions\n",
    "\n",
    "1. Highlight **full sentences** that indicate or describe political corruption — even if indirect or ambiguous.\n",
    "2. Pay attention to keywords such as: bribery, fraud, abuse of power, nepotism, embezzlement, etc.\n",
    "3. Use your judgment to decide whether political corruption is the **main focus** of the article.\n",
    "4. Then, provide:\n",
    "   - A list of the most relevant sentence highlights\n",
    "   - A **tentative label**: Yes / Mentioned but not central / No / Unsure\n",
    "   - A **brief explanation** of your reasoning\n",
    "   - A **confidence score** from 0–100\n",
    "\n",
    "### Output Format\n",
    "\n",
    "Highlights:\n",
    "- [Sentence 1]\n",
    "- [Sentence 2]\n",
    "...\n",
    "\n",
    "Tentative Label: Yes / Mentioned but not central / No / Unsure  \n",
    "Reasoning: [Your explanation]  \n",
    "Confidence: [0–100]\n",
    "\n",
    "---\n",
    "\n",
    "Article:\n",
    "{article_text}\n",
    "\n",
    "Assistant Output:\"\"\"\n",
    "\n",
    "\n",
    "# ========= LLM Request ==========\n",
    "def classify_article(article_text: str) -> dict:\n",
    "    prompt = build_detailed_prompt(article_text)\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            LLM_ENDPOINT,\n",
    "            json={\n",
    "                \"model\": LLM_MODEL_NAME,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        answer = result.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "        # Parse output\n",
    "        highlights = []\n",
    "        tentative_label = \"Unclear\"\n",
    "        rationale_lines = []\n",
    "        confidence = None\n",
    "\n",
    "        lines = answer.splitlines()\n",
    "        reading_highlights = False\n",
    "        reading_rationale = False\n",
    "\n",
    "        for line in lines:\n",
    "            line_strip = line.strip()\n",
    "\n",
    "            # Highlights\n",
    "            if line_strip.lower() == \"highlights:\":\n",
    "                reading_highlights = True\n",
    "                reading_rationale = False\n",
    "                continue\n",
    "            elif line_strip.lower().startswith(\"tentative label:\"):\n",
    "                reading_highlights = False\n",
    "                reading_rationale = False\n",
    "                val = line_strip.split(\":\", 1)[1].strip().capitalize()\n",
    "                if val in [\"Yes\", \"No\", \"Unsure\"]:\n",
    "                    tentative_label = val\n",
    "                continue\n",
    "            elif line_strip.lower().startswith(\"reasoning:\"):\n",
    "                reading_highlights = False\n",
    "                reading_rationale = True\n",
    "                rationale_lines.append(line_strip.split(\":\", 1)[1].strip())\n",
    "                continue\n",
    "            elif line_strip.lower().startswith(\"confidence:\"):\n",
    "                reading_highlights = False\n",
    "                reading_rationale = False\n",
    "                match = re.search(r\"\\d{1,3}\", line_strip)\n",
    "                if match:\n",
    "                    confidence = int(match.group(0))\n",
    "                continue\n",
    "\n",
    "            # Accumulate content\n",
    "            if reading_highlights and line_strip.startswith(\"- \"):\n",
    "                highlights.append(line_strip[2:].strip())\n",
    "            elif reading_rationale:\n",
    "                if line_strip:\n",
    "                    rationale_lines.append(line_strip)\n",
    "\n",
    "        rationale = \" \".join(rationale_lines).strip()\n",
    "\n",
    "        return {\n",
    "            \"tentative_label\": tentative_label,\n",
    "            \"rationale\": rationale,\n",
    "            \"confidence\": confidence,\n",
    "            \"highlights\": highlights\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Classification error: {e}\")\n",
    "        return {\n",
    "            \"tentative_label\": \"Error\",\n",
    "            \"rationale\": str(e),\n",
    "            \"confidence\": None,\n",
    "            \"highlights\": []\n",
    "        }\n",
    "\n",
    "\n",
    "# ========= Highlight Helper ==========\n",
    "def highlight_translated_text(text: str, highlights: List[str]) -> str:\n",
    "    \"\"\"Insert <highlight> tags around matched highlight sentences in the text.\"\"\"\n",
    "    used = set()\n",
    "    for hl in highlights:\n",
    "        pattern = re.escape(hl.strip())\n",
    "        if not pattern or pattern.lower() in used:\n",
    "            continue\n",
    "        regex = re.compile(pattern, re.IGNORECASE)\n",
    "        text, count = regex.subn(r\"<highlight>\\g<0></highlight>\", text, count=1)\n",
    "        if count > 0:\n",
    "            used.add(pattern.lower())\n",
    "    return text\n",
    "\n",
    "KEY_TERMS = [\n",
    "    \"bribery\", \"embezzlement\", \"nepotism\", \"corruption\", \"fraud\",\n",
    "    \"abuse of power\", \"favoritism\", \"money laundering\", \"kickback\", \"cronyism\"\n",
    "]\n",
    "\n",
    "def highlight_keywords(text: str, terms: List[str]) -> str:\n",
    "    for term in terms:\n",
    "        pattern = re.compile(rf\"(?<!<highlight>)(\\b{re.escape(term)}\\b)\", re.IGNORECASE)\n",
    "        text = pattern.sub(r\"<highlight>\\1</highlight>\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# ========= Main Workflow ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"🔍 Loading translated articles from {TRANSLATED_FILE}...\")\n",
    "    df = pd.read_csv(TRANSLATED_FILE)\n",
    "\n",
    "    print(\"🧠 Generating LLM suggestions for annotation support...\")\n",
    "    results = []\n",
    "    highlighted_texts = []\n",
    "\n",
    "    for text in tqdm(df[\"translated_text\"].astype(str), desc=\"Processing articles\"):\n",
    "        result = classify_article(text)\n",
    "        highlighted = highlight_translated_text(text, result[\"highlights\"])\n",
    "        highlighted = highlight_keywords(highlighted, KEY_TERMS)\n",
    "        highlighted_texts.append(highlighted)\n",
    "        results.append(result)\n",
    "\n",
    "    # Merge LLM results into DataFrame\n",
    "    df[\"translated_text\"] = highlighted_texts\n",
    "    df[\"tentative_label\"] = [r[\"tentative_label\"] for r in results]\n",
    "    df[\"llm_confidence\"] = [r[\"confidence\"] for r in results]\n",
    "    df[\"llm_rationale\"] = [r[\"rationale\"] for r in results]\n",
    "    df[\"llm_evidence\"] = [\"; \".join(r[\"highlights\"]) for r in results]\n",
    "    df[\"human_label\"] = \"\"\n",
    "    df[\"qualtrics_qid\"] = [f\"Q{i+1}\" for i in range(len(df))]\n",
    "\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"✅ Saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be980e6-7592-4032-9744-01b8f8eec8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to: sample_with_llm_suggestions.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_output_file = OUTPUT_FILE.replace(\".csv\", \".xlsx\")\n",
    "df.to_excel(excel_output_file, index=False)\n",
    "print(f\"✅ Saved to: {excel_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
