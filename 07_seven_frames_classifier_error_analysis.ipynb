{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "910b1d17-3bd1-4688-9c29-32f0401ccb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS ONE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5de11a88-1152-4754-af4b-46cb061a30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re \n",
    "import re\n",
    "\n",
    "import json# <-- Make sure this is included at the top!\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "LLM_ENDPOINT = \"http://localhost:11434/api/chat\"\n",
    "LLM_MODEL_NAME = \"llama3:70b\"\n",
    "TEMP_OUTPUT_PATH = \"annotated_temp_output.csv\"\n",
    "FINAL_OUTPUT_PATH = \"news_sample_annotated.csv\"\n",
    "SLEEP_BETWEEN_REQUESTS = 1\n",
    "PROMPT_DIR = \"prompts\"\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "# ========== FRAME ORDER ==========\n",
    "FRAME_ORDER = [\n",
    "    \"Foreign influence threat\",\n",
    "    \"Systemic institutional corruption\",\n",
    "    \"Elite collusion\",\n",
    "    \"Politicized investigations\",\n",
    "    \"Authoritarian reformism\",\n",
    "    \"Judicial and institutional accountability failures\",\n",
    "    \"Mobilizing anti-corruption\"\n",
    "]\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def strip_markdown_fences(output):\n",
    "    \"\"\"Remove markdown-style code fences from model output.\"\"\"\n",
    "    return re.sub(r\"```(?:json)?|```\", \"\", output).strip()\n",
    "\n",
    "\n",
    "def extract_json_from_output(output):\n",
    "    \"\"\"\n",
    "    Safely extract a JSON array from model output.\n",
    "    Handles extra text, markdown, and formatting errors.\n",
    "    \"\"\"\n",
    "    output = strip_markdown_fences(output)\n",
    "\n",
    "    try:\n",
    "        # Try parsing directly\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            # Try extracting just the first JSON array\n",
    "            json_str = re.search(r'\\[\\s*{.*?}\\s*\\]', output, re.DOTALL).group()\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå JSON decode failed: {e}\")\n",
    "            print(\"üîç Raw output that caused error (truncated):\")\n",
    "            print(output[:500])\n",
    "            return None\n",
    "            \n",
    "# ========== LOAD PROMPTS PER FRAME ==========\n",
    "def load_frame_prompt(index: int, frame_name: str) -> str:\n",
    "    filename = f\"frame_{index}_{frame_name.lower().replace(' ', '_').replace('-', '')}.txt\"\n",
    "    path = os.path.join(PROMPT_DIR, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# ========== PROMPT CONSTRUCTION ==========\n",
    "def build_prompt(article_text: str, frame_index: int, frame_name: str) -> str:\n",
    "    frame_prompt = load_frame_prompt(frame_index, frame_name)\n",
    "    return f\"{frame_prompt}\\n\\n---\\n\\nArticle:\\n{article_text}\"\n",
    "\n",
    "def sanitize_double_quotes(json_str):\n",
    "    \"\"\"\n",
    "    Fix common LLM quote formatting issues like nested double quotes.\n",
    "    Turns:  \"evidence\": \"\"This will...\" ‚Üí \"evidence\": \"This will...\"\n",
    "    \"\"\"\n",
    "    return re.sub(r'\"\\s*\"\\s*([^\"]+)\"', r'\"\\1\"', json_str)\n",
    "\n",
    "def clean_llm_response(content):\n",
    "    \"\"\"\n",
    "    Cleans and parses LLM response content into a JSON object.\n",
    "    Handles:\n",
    "    - Markdown code fences\n",
    "    - Overquoted fields\n",
    "    - Stray text around JSON array\n",
    "    - Common decode errors\n",
    "    \"\"\"\n",
    "    content = content.strip()\n",
    "    content = re.sub(r\"```(?:json)?|```\", \"\", content).strip()\n",
    "\n",
    "    # First try: parse whole response as JSON\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Second try: extract the first array-like JSON block\n",
    "    match = re.search(r'\\[\\s*{.*?}\\s*\\]', content, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group()\n",
    "        json_str = sanitize_double_quotes(json_str)\n",
    "\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è Still couldn't parse extracted JSON: {e}\")\n",
    "            print(\"üîç JSON candidate (truncated):\")\n",
    "            print(json_str[:500])\n",
    "            return None\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No JSON array found in LLM output.\")\n",
    "        print(\"üîç Raw LLM output (truncated):\")\n",
    "        print(content[:1000])\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_frame_llm(article_text: str, frame_index: int, frame_name: str) -> dict:\n",
    "    prompt = build_prompt(article_text, frame_index, frame_name)\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            LLM_ENDPOINT,\n",
    "            json={\n",
    "                \"model\": LLM_MODEL_NAME,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False,\n",
    "                \"temperature\": TEMPERATURE\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        content = result.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "\n",
    "        if not content:\n",
    "            raise ValueError(\"Empty response from LLM\")\n",
    "\n",
    "        parsed = clean_llm_response(content)\n",
    "        if not parsed or not isinstance(parsed, list):\n",
    "            raise ValueError(\"No valid JSON array found or parsed content is not a list\")\n",
    "\n",
    "        return parsed[0] if parsed else {}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying frame '{frame_name}': {e}\")\n",
    "        return {\n",
    "            \"frame\": frame_name,\n",
    "            \"rationale\": f\"‚ö†Ô∏è Error: {str(e)}\",\n",
    "            \"confidence\": None,\n",
    "            \"evidence\": \"\"\n",
    "        }\n",
    "\n",
    "\n",
    "# ========== ANNOTATION LOOP ==========\n",
    "def annotate_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for i in range(1, len(FRAME_ORDER) + 1):\n",
    "        for field in [\"name\", \"rationale\", \"confidence\", \"evidence\"]:\n",
    "            col = f\"frame_{i}_{field}\"\n",
    "            if col not in df.columns:\n",
    "                df[col] = \"\"\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row.get(\"frame_1_name\")) and row.get(\"frame_1_name\") != \"\":\n",
    "            print(f\"‚è≠Ô∏è Article {idx} already annotated. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Annotating article {idx}...\")\n",
    "        article_text = row.get(\"translated_text\", \"\")\n",
    "\n",
    "        for i, frame_name in enumerate(FRAME_ORDER, 1):\n",
    "            result = query_frame_llm(article_text, i, frame_name)\n",
    "\n",
    "            # Only log frames with rationale and frame label\n",
    "            if result.get(\"frame\") and result.get(\"rationale\"):\n",
    "                confidence = result.get(\"confidence\", \"\")\n",
    "                rationale = result.get(\"rationale\", \"\")\n",
    "                if confidence != \"\" and isinstance(confidence, (int, float)) and confidence < 80:\n",
    "                    rationale += f\"\\n\\n‚ö†Ô∏è Model confidence is only {confidence}%. Please verify carefully.\"\n",
    "\n",
    "                df.at[idx, f\"frame_{i}_name\"] = result.get(\"frame\", \"\")\n",
    "                df.at[idx, f\"frame_{i}_rationale\"] = rationale\n",
    "                df.at[idx, f\"frame_{i}_confidence\"] = confidence\n",
    "                df.at[idx, f\"frame_{i}_evidence\"] = result.get(\"evidence\", \"\")\n",
    "\n",
    "\n",
    "        df.to_csv(TEMP_OUTPUT_PATH, index=False)\n",
    "        print(f\"‚úÖ Saved progress after article {idx}.\")\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8f30bf78-087d-4613-a56d-41e552fa0f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded existing raw ICR2 sample from: /home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_sample_raw.csv\n",
      "\n",
      "üîç Annotating article 0...\n",
      "‚úÖ Saved progress after article 0.\n",
      "\n",
      "üîç Annotating article 1...\n",
      "‚úÖ Saved progress after article 1.\n",
      "\n",
      "üîç Annotating article 2...\n",
      "‚úÖ Saved progress after article 2.\n",
      "\n",
      "üîç Annotating article 3...\n",
      "‚úÖ Saved progress after article 3.\n",
      "\n",
      "üîç Annotating article 4...\n",
      "‚úÖ Saved progress after article 4.\n",
      "\n",
      "üîç Annotating article 5...\n",
      "‚úÖ Saved progress after article 5.\n",
      "\n",
      "üîç Annotating article 6...\n",
      "‚úÖ Saved progress after article 6.\n",
      "\n",
      "üîç Annotating article 7...\n",
      "‚úÖ Saved progress after article 7.\n",
      "\n",
      "üîç Annotating article 8...\n",
      "‚úÖ Saved progress after article 8.\n",
      "\n",
      "üîç Annotating article 9...\n",
      "‚úÖ Saved progress after article 9.\n",
      "\n",
      "üîç Annotating article 10...\n",
      "‚úÖ Saved progress after article 10.\n",
      "\n",
      "üîç Annotating article 11...\n",
      "‚úÖ Saved progress after article 11.\n",
      "\n",
      "üîç Annotating article 12...\n",
      "‚úÖ Saved progress after article 12.\n",
      "\n",
      "üîç Annotating article 13...\n",
      "‚úÖ Saved progress after article 13.\n",
      "\n",
      "üîç Annotating article 14...\n",
      "‚úÖ Saved progress after article 14.\n",
      "‚úÖ Annotated ICR 2 saved to: /home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_sample_LLM_annotated.csv\n",
      "‚úÖ Excel-bestand ICR 2 opgeslagen als: /home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_sample_LLM_annotated.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your existing raw ICR2 sample\n",
    "icr2_sample_path = os.path.expanduser(\n",
    "    \"~/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_sample_raw.csv\"\n",
    ")\n",
    "\n",
    "# Load the already drawn raw sample\n",
    "df_icr2 = pd.read_csv(icr2_sample_path)\n",
    "print(f\"üìÇ Loaded existing raw ICR2 sample from: {icr2_sample_path}\")\n",
    "\n",
    "#Annotate\n",
    "df_icr2 = annotate_dataframe(df_icr2)\n",
    "\n",
    "# Save annotated ICR 2 sample\n",
    "icr2_annotated_path = os.path.expanduser(\n",
    "    \"~/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_sample_LLM_annotated.csv\"\n",
    ")\n",
    "df_icr2.to_csv(icr2_annotated_path, index=False)\n",
    "print(f\"‚úÖ Annotated ICR 2 saved to: {icr2_annotated_path}\")\n",
    "\n",
    "# Save Excel version of ICR 2\n",
    "icr2_excel_path = icr2_annotated_path.replace(\".csv\", \".xlsx\")\n",
    "df_icr2.to_excel(icr2_excel_path, index=False)\n",
    "print(f\"‚úÖ Excel-bestand ICR 2 opgeslagen als: {icr2_excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4a6b1994-b208-40e4-b46a-891a10411f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [\n",
    "    \"~/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/output/data-deductive-analysis/sample-manual-content-analysis/Bulgaria_Alexander_sample_250.csv\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970ebd6-feed-42c3-ae07-6ad1007fe6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "icr3_sample_path = os.path.expanduser(\n",
    "    \"~/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test3/icr3_sample_raw.csv\"\n",
    ")\n",
    "df_icr3 = pd.read_csv(icr3_sample_path)\n",
    "print(f\"üìÇ Loaded existing raw ICR2 sample from: {icr3_sample_path}\")\n",
    "\n",
    "df_icr3 = annotate_dataframe(df_icr3)\n",
    "\n",
    "df_icr3.to_csv(icr3_annotated_path, index=False)\n",
    "print(f\"‚úÖ Annotated ICR 3 saved to: {icr3_annotated_path}\")\n",
    "\n",
    "# Save Excel version of ICR 2\n",
    "icr3_excel_path = icr3_annotated_path.replace(\".csv\", \".xlsx\")\n",
    "df_icr3.to_excel(icr3_excel_path, index=False)\n",
    "print(f\"‚úÖ Excel-bestand ICR 3opgeslagen als: {icr3_excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b90599f2-c0dc-4b48-a863-b7561ba023f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HTML report saved to: /home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_annotated_report.html\n",
      "‚úÖ HTML report saved to: /home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test3/icr3_annotated_report.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def generate_annotation_html(df, output_path):\n",
    "    html_content = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Annotated Articles Report</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }\n",
    "            .article { margin-bottom: 50px; padding-bottom: 20px; border-bottom: 1px solid #ccc; }\n",
    "            .frame { margin-left: 20px; margin-top: 10px; }\n",
    "            .header { font-size: 20px; font-weight: bold; color: #333; }\n",
    "            .label { font-weight: bold; color: #004080; }\n",
    "            .section { margin-top: 10px; }\n",
    "            .meta { color: #555; font-size: 14px; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>üìù Annotated Articles with Frames & Rationales</h1>\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        html_content += f'<div class=\"article\">'\n",
    "        html_content += f'<div class=\"header\">üìÑ Article #{idx + 1}</div>'\n",
    "        html_content += f'<div class=\"meta\">üåç <b>Country:</b> {row.get(\"country\", \"N/A\")} &nbsp;&nbsp; üïí <b>DateTime:</b> {row.get(\"dateTime\", \"N/A\")} &nbsp;&nbsp; üîó <b>Source:</b> {row.get(\"source.uri\", \"N/A\")}</div>'\n",
    "        html_content += f'<div class=\"section\"><span class=\"label\">üì∞ Text:</span><br>{row.get(\"translated_text\", \"N/A\")}</div>'\n",
    "\n",
    "        for n in range(1, 8):\n",
    "            frame_name = row.get(f'frame_{n}_name', '')\n",
    "            rationale = row.get(f'frame_{n}_rationale', '')\n",
    "            evidence = row.get(f'frame_{n}_evidence', '')\n",
    "            confidence = row.get(f'frame_{n}_confidence', 'N/A')\n",
    "\n",
    "            # Skip if frame is None, empty, or evidence is missing\n",
    "            if (\n",
    "                pd.notna(frame_name) and frame_name.strip().lower() != \"none\" and frame_name.strip() and\n",
    "                pd.notna(evidence) and evidence.strip()\n",
    "            ):\n",
    "                html_content += f'<div class=\"frame\">'\n",
    "                html_content += f'<div><span class=\"label\">üóÇÔ∏è Frame {n}:</span> {frame_name}</div>'\n",
    "                if pd.notna(rationale) and rationale.strip():\n",
    "                    html_content += f'<div><span class=\"label\">üí° Rationale:</span> {rationale}</div>'\n",
    "                html_content += f'<div><span class=\"label\">üîç Evidence:</span> {evidence}</div>'\n",
    "                html_content += f'<div><span class=\"label\">üìà Confidence:</span> {confidence}</div>'\n",
    "                html_content += f'</div>'\n",
    "\n",
    "        html_content += '</div>'\n",
    "\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"‚úÖ HTML report saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage (adjust paths and dataframes accordingly)\n",
    "output_file = os.path.expanduser(\"/home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_annotated_report.html\")\n",
    "generate_annotation_html(df_icr2, output_file)\n",
    "\n",
    "output_file = os.path.expanduser(\"/home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test3/icr3_annotated_report.html\")\n",
    "generate_annotation_html(df_icr3, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f513b7-f09d-4734-9210-9b5c7674ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation ===\n"
     ]
    }
   ],
   "source": [
    "# ======== COMPARE TO YARA ========\n",
    "print(\"\\n=== Evaluation ===\")\n",
    "gold = pd.read_csv(GOLD_STANDARD_PATH).set_index(\"uri\")\n",
    "updated = pd.read_csv(UPDATED_OUTPUT_PATH).set_index(\"uri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3ddca-44a9-4c8b-8475-d4feeb6df95d",
   "metadata": {},
   "source": [
    "## ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "275c3130-60e3-46c8-8f6c-28430396bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Annotating row 0\n",
      "üîç Annotating row 1\n",
      "üîç Annotating row 2\n",
      "üîç Annotating row 3\n",
      "üîç Annotating row 4\n",
      "üîç Annotating row 5\n",
      "üîç Annotating row 6\n",
      "üîç Annotating row 7\n",
      "üîç Annotating row 8\n",
      "üîç Annotating row 9\n",
      "üîç Annotating row 10\n",
      "üîç Annotating row 11\n",
      "üîç Annotating row 12\n",
      "üîç Annotating row 13\n",
      "üîç Annotating row 14\n",
      "‚úÖ Saved LLM results to /home/akroon/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/single_frame_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ======== CONFIG ========\n",
    "LLM_ENDPOINT = \"http://localhost:11434/api/chat\"\n",
    "LLM_MODEL_NAME = \"llama3:70b\"\n",
    "#FRAME_INDEX = 2\n",
    "#FRAME_INDEX = 3\n",
    "#FRAME_INDEX = 4\n",
    "#FRAME_INDEX = 5\n",
    "FRAME_INDEX = 6\n",
    "#FRAME_NAME = \"Systemic institutional corruption\"\n",
    "#FRAME_NAME = \"Elite collusion\"\n",
    "#FRAME_NAME = \"Politicized investigations\"\n",
    "#FRAME_NAME = \"Authoritarian reformism\"\n",
    "FRAME_NAME = 'Judicial and institutional accountability failures'\n",
    "PROMPT_DIR = \"prompts\"\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "SESSION_FOLDER = os.path.expanduser(\"~/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/sessions/\")\n",
    "\n",
    "INPUT_PATH = os.path.expanduser(\n",
    "    \"~/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/icr2_sample_raw.csv\"\n",
    ")\n",
    "UPDATED_OUTPUT_PATH = os.path.expanduser(\n",
    "    \"~/webdav/ASCOR-FMG-5580-RESPOND-news-data (Projectfolder)/annotations/coding_frames/ICR/ICR_test2/single_frame_updated.csv\"\n",
    ")\n",
    "\n",
    "# ======== FRAME COLUMN MAPPING ========\n",
    "FRAME_COLUMN = {\n",
    "    \"Foreign influence threat\": \"Foreign influence threat_present\",\n",
    "    \"Systemic institutional corruption\": \"Systemic institutional corruption_present\",\n",
    "    \"Elite collusion\": \"Elite collusion_present\",\n",
    "    \"Politicized investigations\": \"Politicized investigations_present\",\n",
    "    \"Authoritarian reformism\": \"Authoritarian reformism_present\",\n",
    "    \"Judicial and institutional accountability failures\": \"Judicial and institutional accountability failures_present\",\n",
    "    \"Mobilizing anti-corruption\": \"Mobilizing anti-corruption_present\"\n",
    "}[FRAME_NAME]\n",
    "\n",
    "# ======== HELPERS ========\n",
    "def encode_label(val):\n",
    "    return 1 if val == \"Present\" else 0 if val == \"Not Present\" else None\n",
    "\n",
    "def load_prompt(index, name):\n",
    "    filename = f\"frame_{index}_{name.lower().replace(' ', '_').replace('-', '')}.txt\"\n",
    "    path = os.path.join(PROMPT_DIR, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def build_prompt(article_text):\n",
    "    prompt_text = load_prompt(FRAME_INDEX, FRAME_NAME)\n",
    "    return f\"{prompt_text}\\n\\n---\\n\\nArticle:\\n{article_text}\"\n",
    "\n",
    "def query_llm(article_text):\n",
    "    prompt = build_prompt(article_text)\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            LLM_ENDPOINT,\n",
    "            json={\n",
    "                \"model\": LLM_MODEL_NAME,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False,\n",
    "                \"temperature\": TEMPERATURE\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        content = response.json().get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "        json_start = content.find(\"[\")\n",
    "        json_end = content.rfind(\"]\") + 1\n",
    "        parsed = json.loads(content[json_start:json_end])\n",
    "        return parsed[0] if parsed else {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {e}\")\n",
    "        return {\"frame\": \"None\", \"rationale\": str(e), \"confidence\": None}\n",
    "\n",
    "# ======== LLM ANNOTATION ========\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "col_name = f\"frame_{FRAME_INDEX}_name\"\n",
    "df[col_name] = \"\"\n",
    "df[f\"frame_{FRAME_INDEX}_rationale\"] = \"\"\n",
    "df[f\"frame_{FRAME_INDEX}_confidence\"] = \"\"\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"üîç Annotating row {idx}\")\n",
    "    result = query_llm(row[\"translated_text\"])\n",
    "    df.at[idx, col_name] = result.get(\"frame\", \"\")\n",
    "    df.at[idx, f\"frame_{FRAME_INDEX}_rationale\"] = result.get(\"rationale\", \"\")\n",
    "    df.at[idx, f\"frame_{FRAME_INDEX}_confidence\"] = result.get(\"confidence\", \"\")\n",
    "\n",
    "df.to_csv(UPDATED_OUTPUT_PATH, index=False)\n",
    "print(f\"‚úÖ Saved LLM results to {UPDATED_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cc0dae88-04a3-4faf-aaa1-9d6d8c2a6fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping unknown annotator: Anne\n",
      "‚úÖ Annotators who coded all 15 articles: ['Yara']\n",
      "\n",
      "=== Evaluation against Yara ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Present       1.00      0.92      0.96        12\n",
      "     Present       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.93        15\n",
      "   macro avg       0.88      0.96      0.91        15\n",
      "weighted avg       0.95      0.93      0.94        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======== LOAD YARA'S GOLD STANDARD FROM JSON ========\n",
    "def load_all_annotations():\n",
    "    data = []\n",
    "\n",
    "    allowed_annotators = [\"Assia\", \"Alexander\", \"Elisa\", \"Luigia\", \"Yara\"]\n",
    "    allowed_set = set(a.lower() for a in allowed_annotators)\n",
    "\n",
    "    for filename in os.listdir(SESSION_FOLDER):\n",
    "        if filename.endswith(\"_session_icr2.json\"):\n",
    "            user_id = filename.replace(\"_session_icr2.json\", \"\")\n",
    "            user_id_lower = user_id.lower()\n",
    "\n",
    "            if user_id_lower not in allowed_set:\n",
    "                print(f\"‚ö†Ô∏è Skipping unknown annotator: {user_id}\")\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(SESSION_FOLDER, filename)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                session_data = json.load(f)\n",
    "                annotations = session_data.get(\"annotations\", [])\n",
    "                for ann in annotations:\n",
    "                    ann[\"user_id\"] = user_id\n",
    "                    data.append(ann)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    annotator_counts = df.groupby(\"user_id\")[\"uri\"].nunique()\n",
    "    complete_annotators = annotator_counts[annotator_counts == 15].index.tolist()\n",
    "    df = df[df[\"user_id\"].isin(complete_annotators)]\n",
    "\n",
    "    print(f\"‚úÖ Annotators who coded all 15 articles: {complete_annotators}\")\n",
    "    return df\n",
    "\n",
    "# ======== COMPARE TO YARA ========\n",
    "annotations_df = load_all_annotations()\n",
    "yara_df = annotations_df[annotations_df[\"user_id\"].str.lower() == \"yara\"]\n",
    "\n",
    "# Get Yara's label for this frame\n",
    "yara_df[\"label\"] = yara_df[FRAME_COLUMN].apply(encode_label)\n",
    "yara_gold = yara_df[[\"uri\", \"label\"]].dropna().set_index(\"uri\")\n",
    "\n",
    "# Load LLM predictions\n",
    "llm_df = pd.read_csv(UPDATED_OUTPUT_PATH).set_index(\"uri\")\n",
    "llm_pred = llm_df[f\"frame_{FRAME_INDEX}_name\"].fillna(\"\").apply(\n",
    "    lambda x: 1 if x.strip().lower() == FRAME_NAME.lower() else 0\n",
    ")\n",
    "\n",
    "# Align and compare\n",
    "aligned = yara_gold.join(llm_pred.rename(\"llm_pred\"), how=\"inner\")\n",
    "\n",
    "print(\"\\n=== Evaluation against Yara ===\")\n",
    "if aligned.empty:\n",
    "    print(\"‚ùå No overlapping URIs found between LLM and Yara annotations.\")\n",
    "else:\n",
    "    print(classification_report(aligned[\"label\"], aligned[\"llm_pred\"], target_names=[\"Not Present\", \"Present\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e09367bc-8c18-419a-96c3-b99b1cfed66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping unknown annotator: Anne\n",
      "‚úÖ Annotators who coded all 15 articles: ['Yara']\n",
      "\n",
      "üìä False Positives:\n",
      "\n",
      "================================================================================\n",
      "üîé Error Type:        False Positive\n",
      "üìÑ Article:\n",
      "They detained the father of an associate of Navalny ‚Äì Information Agency PIK\n",
      "\n",
      "An associate of Russian oppositionist Alexei Navalny accused the Russian authorities of detaining his father in an attempt to exert pressure on him.\n",
      "\n",
      "Ivan Zhdanov, director of the Anti-Corruption Foundation founded by Navalny, announced that the police conducted a midnight raid on his father‚Äôs home in Rostov-on-Don on suspicion of abuse of power and took him for questioning.\n",
      "\n",
      "‚ÄúI have no doubts that this criminal case is connected to me and what I do,‚Äù Zhdanov, who is outside Russia as part of a group of opponents of the Kremlin, who are trying to gather support for new protests this spring against the regime in Russia, said.\n",
      "\n",
      "According to Zhdanov, his 66-year-old father is a pensioner with various health problems and will find it difficult to survive in pre-trial detention, BTA reports.\n",
      "\n",
      "Navalny himself, who is serving a sentence in prison, announced in a post on Instagram that he is likely to be sent to pris\n",
      "\n",
      "ü§ñ LLM Prediction:   1 (Confidence: 95.0)\n",
      "üìù LLM Rationale:\n",
      "The police detained Ivan Zhdanov's father on suspicion of abuse of power, which is likely a form of political pressure on Navalny's associate, showing selective enforcement and institutional capture.\n",
      "‚úÖ Yara Label:        0\n",
      "================================================================================\n",
      "\n",
      "\n",
      "üìä False Negatives:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======== Extract disagreement cases for prompt improvement ========\n",
    "import pandas as pd\n",
    "\n",
    "# Reload all to ensure synced state\n",
    "llm_df = pd.read_csv(UPDATED_OUTPUT_PATH)\n",
    "llm_df.set_index(\"uri\", inplace=True)\n",
    "\n",
    "# Add prediction and confidence fields\n",
    "llm_df[\"llm_pred\"] = llm_df[f\"frame_{FRAME_INDEX}_name\"].fillna(\"\").apply(\n",
    "    lambda x: 1 if x.strip().lower() == FRAME_NAME.lower() else 0\n",
    ")\n",
    "llm_df[\"confidence\"] = pd.to_numeric(llm_df[f\"frame_{FRAME_INDEX}_confidence\"], errors=\"coerce\")\n",
    "\n",
    "# Gold labels from Yara\n",
    "annotations_df = load_all_annotations()\n",
    "yara_df = annotations_df[annotations_df[\"user_id\"].str.lower() == \"yara\"]\n",
    "yara_df[\"label\"] = yara_df[FRAME_COLUMN].apply(encode_label)\n",
    "yara_gold = yara_df[[\"uri\", \"label\"]].dropna().set_index(\"uri\")\n",
    "\n",
    "# Align LLM + Yara labels\n",
    "aligned = yara_gold.join(llm_df, how=\"inner\")\n",
    "\n",
    "# Extract mismatches\n",
    "false_positives = aligned[(aligned[\"label\"] == 0) & (aligned[\"llm_pred\"] == 1)]\n",
    "false_negatives = aligned[(aligned[\"label\"] == 1) & (aligned[\"llm_pred\"] == 0)]\n",
    "\n",
    "# Combine for export\n",
    "mismatches = pd.concat([false_positives, false_negatives])\n",
    "mismatches[\"error_type\"] = mismatches.apply(\n",
    "    lambda row: \"False Positive\" if row[\"label\"] == 0 else \"False Negative\", axis=1\n",
    ")\n",
    "\n",
    "# Rename columns for readability\n",
    "mismatches_export = mismatches[[\n",
    "    \"translated_text\",\n",
    "    f\"frame_{FRAME_INDEX}_rationale\",\n",
    "    \"confidence\",\n",
    "    \"llm_pred\",\n",
    "    \"label\",\n",
    "    \"error_type\"\n",
    "]].rename(columns={\n",
    "    \"translated_text\": \"article\",\n",
    "    f\"frame_{FRAME_INDEX}_rationale\": \"llm_rationale\",\n",
    "    \"confidence\": \"llm_confidence\",\n",
    "    \"label\": \"yara_label\",\n",
    "    \"llm_pred\": \"llm_prediction\"\n",
    "})\n",
    "# Print mismatches for manual inspection\n",
    "def print_disagreements(df, num=7):\n",
    "    def pretty_print(row):\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üîé Error Type:        {row['error_type']}\")\n",
    "        print(f\"üìÑ Article:\\n{row['article'][:1000]}\")  # truncate if long\n",
    "        print(f\"\\nü§ñ LLM Prediction:   {row['llm_prediction']} (Confidence: {row['llm_confidence']})\")\n",
    "        print(f\"üìù LLM Rationale:\\n{row['llm_rationale']}\")\n",
    "        print(f\"‚úÖ Yara Label:        {row['yara_label']}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(\"\\nüìä False Positives:\\n\")\n",
    "    for _, row in mismatches_export[mismatches_export[\"error_type\"] == \"False Positive\"].head(num).iterrows():\n",
    "        pretty_print(row)\n",
    "\n",
    "    print(\"\\nüìä False Negatives:\\n\")\n",
    "    for _, row in mismatches_export[mismatches_export[\"error_type\"] == \"False Negative\"].head(num).iterrows():\n",
    "        pretty_print(row)\n",
    "\n",
    "# Call it\n",
    "print_disagreements(mismatches_export, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b5bf9ec-94ef-4cf4-8a33-09e1f7643272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping unknown annotator: Anne\n",
      "‚ö†Ô∏è Skipping unknown annotator: anne\n",
      "‚ö†Ô∏è Skipping unknown annotator: code1\n",
      "‚ö†Ô∏è Skipping unknown annotator: coder1\n",
      "‚ö†Ô∏è Skipping unknown annotator: frozen\n",
      "‚ö†Ô∏è Skipping unknown annotator: simone\n",
      "‚úÖ Annotators who coded all 15 articles: ['Alexander', 'Assia', 'Elisa', 'Luigia', 'Yara']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3693366/993280933.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  yara_df[\"label\"] = yara_df[FRAME_COLUMN].apply(encode_label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation against Yara ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Present       0.90      0.75      0.82        12\n",
      "     Present       0.40      0.67      0.50         3\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.65      0.71      0.66        15\n",
      "weighted avg       0.80      0.73      0.75        15\n",
      "\n",
      "\n",
      "=== Confidence Analysis ===\n",
      "‚úÖ True Positives: 2 | Avg Confidence: 95.0\n",
      "‚úÖ True Negatives: 9 | Avg Confidence: 87.1\n",
      "‚ùå False Positives: 3 | Avg Confidence: 90.7\n",
      "‚ùå False Negatives: 1 | Avg Confidence: 85.0\n",
      "\n",
      "üîç False Positives:\n",
      "            llm_pred  label  confidence\n",
      "uri                                    \n",
      "6571925780         1      0        95.0\n",
      "7931240488         1      0        92.0\n",
      "7940552433         1      0        85.0\n",
      "\n",
      "üîç False Negatives:\n",
      "            llm_pred  label  confidence\n",
      "uri                                    \n",
      "6816585638         0      1        85.0\n"
     ]
    }
   ],
   "source": [
    "# ======== COMPARE TO YARA WITH CONFIDENCE ANALYSIS ========\n",
    "import numpy as np\n",
    "\n",
    "annotations_df = load_all_annotations()\n",
    "yara_df = annotations_df[annotations_df[\"user_id\"].str.lower() == \"yara\"]\n",
    "\n",
    "# Get Yara's label for this frame\n",
    "yara_df[\"label\"] = yara_df[FRAME_COLUMN].apply(encode_label)\n",
    "yara_gold = yara_df[[\"uri\", \"label\"]].dropna().set_index(\"uri\")\n",
    "\n",
    "# Load LLM predictions and confidence\n",
    "llm_df = pd.read_csv(UPDATED_OUTPUT_PATH).set_index(\"uri\")\n",
    "llm_df[\"llm_pred\"] = llm_df[f\"frame_{FRAME_INDEX}_name\"].fillna(\"\").apply(\n",
    "    lambda x: 1 if x.strip().lower() == FRAME_NAME.lower() else 0\n",
    ")\n",
    "llm_df[\"confidence\"] = pd.to_numeric(llm_df[f\"frame_{FRAME_INDEX}_confidence\"], errors=\"coerce\")\n",
    "\n",
    "# Align on shared URIs\n",
    "aligned = yara_gold.join(llm_df[[\"llm_pred\", \"confidence\"]], how=\"inner\")\n",
    "\n",
    "print(\"\\n=== Evaluation against Yara ===\")\n",
    "if aligned.empty:\n",
    "    print(\"‚ùå No overlapping URIs found between LLM and Yara annotations.\")\n",
    "else:\n",
    "    # Classification report\n",
    "    print(classification_report(aligned[\"label\"], aligned[\"llm_pred\"], target_names=[\"Not Present\", \"Present\"]))\n",
    "\n",
    "    # Confidence analysis\n",
    "    print(\"\\n=== Confidence Analysis ===\")\n",
    "\n",
    "    # True Positives\n",
    "    tp = aligned[(aligned[\"label\"] == 1) & (aligned[\"llm_pred\"] == 1)]\n",
    "    # True Negatives\n",
    "    tn = aligned[(aligned[\"label\"] == 0) & (aligned[\"llm_pred\"] == 0)]\n",
    "    # False Positives\n",
    "    fp = aligned[(aligned[\"label\"] == 0) & (aligned[\"llm_pred\"] == 1)]\n",
    "    # False Negatives\n",
    "    fn = aligned[(aligned[\"label\"] == 1) & (aligned[\"llm_pred\"] == 0)]\n",
    "\n",
    "    def avg_conf(df):\n",
    "        return round(df[\"confidence\"].mean(), 1) if not df.empty else \"n/a\"\n",
    "\n",
    "    print(f\"‚úÖ True Positives: {len(tp)} | Avg Confidence: {avg_conf(tp)}\")\n",
    "    print(f\"‚úÖ True Negatives: {len(tn)} | Avg Confidence: {avg_conf(tn)}\")\n",
    "    print(f\"‚ùå False Positives: {len(fp)} | Avg Confidence: {avg_conf(fp)}\")\n",
    "    print(f\"‚ùå False Negatives: {len(fn)} | Avg Confidence: {avg_conf(fn)}\")\n",
    "\n",
    "    # Print mismatches with confidence and optionally rationale\n",
    "    if not fp.empty:\n",
    "        print(\"\\nüîç False Positives:\")\n",
    "        print(fp[[\"llm_pred\", \"label\", \"confidence\"]])\n",
    "\n",
    "    if not fn.empty:\n",
    "        print(\"\\nüîç False Negatives:\")\n",
    "        print(fn[[\"llm_pred\", \"label\", \"confidence\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32af63a-5809-47d1-8828-4a7bbfb27546",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gold_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m llm_labels \u001b[38;5;241m=\u001b[39m updated[col_name]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(\u001b[43mgold_labels\u001b[49m, llm_labels, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot Present\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPresent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gold_labels' is not defined"
     ]
    }
   ],
   "source": [
    "llm_labels = updated[col_name].map(lambda x: 1 if isinstance(x, str) and x.strip() else 0)\n",
    "\n",
    "report = classification_report(gold_labels, llm_labels, target_names=[\"Not Present\", \"Present\"])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
